# RoomTex

This is the implementation of RoomTex: Texturing Compositional Indoor Scenes via Iterative Inpainting.
### [Project Page](https://qwang666.github.io/RoomTex/) | [Paper](https://arxiv.org/abs/)
<div align=center>
<img src="teaser.png" width="100%"/>
</div>

## Installation

Tested on A100, V100. If your GPU memory is not enough, you can reduce the `batch_size`&`batch_count` in `configs` files.

```sh
conda create -n RoomTex python=3.8
conda activate RoomTex
pip install -r requirements.txt
```
_other versions of python and pytorch should also work fine._

## Quickstart

### Stable Diffusion and ControlNet

Deploy `stable-diffusion-webui` as [stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/5121846d34d74aee9b55d48d35c1559a710051b0)

Download SDXL model from 

[Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | 
[Refiner model](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0) | 
[VAE model](https://huggingface.co/stabilityai/sdxl-vae) | 
[SDXL-controlnet depth model](https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0)


Modify the SDXL webui as `sdxl/modify_code/readme.txt`

then, run stable-diiffusion-webui on nowebui mode as
```sh
CUDA_VISIBLE_DEVICES=0 bash webui.sh --nowebui --port 7860
```
you can choose the GPU device by setting `CUDA_VISIBLE_DEVICES` and the port by setting `--port`

Then you can use the stable diffusion through this port

### Room mesh
Example room meshes are provided in `demo/objects/livingroom/` folder. 

Object mesh can also be generated by using the script from [Shap-E](https://github.com/openai/shap-e/blob/main/shap_e/examples/sample_text_to_3d.ipynb) .

Empty room mesh from sketch.
```sh
python utils/mesh/gene_room.py --cfg demo/configs/livingroom.yaml
```
*saved in `demo/objects/`. use generated room, modify the `config` file to add room mesh path on `room_mesh_path` and `boundary_mesh_path`*

Room mesh can also use [3D-FRONT dataset](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset).


### Prepare the panorama and objects inpainting view depth of the scene

```sh
python scripts/prepare_depth.py --cfg demo/configs/livingroom.yaml
```
_saved in `config['save_path']`_

### Generate the panorama of the scene (port is the sd webui port)
```sh
python gene_img/pano/pano_text2img.py --cfg demo/configs/livingroom.yaml --port 7860
```
_saved in `config['save_path'] + '/pano/image'`. choose one and modify the `config` file to add the panorama path on `pano_all_2K`_

### Refine the room panorama

```sh
python gene_img/pano/refine_pano.py --cfg demo/configs/livingroom.yaml --port 7860
```


### Reproject the panorama to initial perspective image
```sh
python scripts/prepare_pers.py
```

### Genetate objects by iterative inpainting

```sh
python scripts/iterative_gene.py --cfg demo/configs/livingroom.yaml --port 7860 --id 0

python scripts/adornment_refine.py --cfg demo/configs/livingroom.yaml --port 7860 --id 0
```
_port is the sd webui port, id is the object id._

_If you have multify GPUs, you can run it parallelly._
_For example, first establish N webui services_

```sh
CUDA_VISIBLE_DEVICES=0 bash webui.sh --nowebui --port 7860
...
CUDA_VISIBLE_DEVICES=N bash webui.sh --nowebui --port N
```
Then run iterative inpainting like:
```sh
python scripts/iterative_gene.py --cfg demo/configs/livingroom.yaml --port N --id n
...
```

### Render figures of the scene, need to set poses
```sh
python scripts/render/render.py
```
_saved in `config['save_path'] + '/Figure'`_

##Citation
```

```